devtools::install_github("r-lib/processx")
devtools::install_github("rstudio/reticulate")
devtools::install_github("rstudio/tensorflow")
devtools::install_github("rstudio/tensorflow")
install.packages("yaml")
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow(gpu = TRUE)
library(tensorflow)
install_tensorflow(version = "gpu")
use_condaenv("r-tensorflow")
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
use_condaenv("r-tensorflow")
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
sess = tf$Session()
library(tensorflow)
use_condaenv("r-tensorflow")
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
library(tensorflow)
library(tensorflow)
install_tensorflow(version = "gpu")
library(tensorflow)
install_tensorflow(version = "gpu")
library(tensorflow)
use_condaenv("r-tensorflow")
sess = tf$Session()
reticulate::py_config()
use_condaenv("r-tensorflow")
sess = tf$Session()
use_condaenv("r-tensorflow")
sess = tf$Session()
reticulate::py_config()
use_condaenv("tensorflow")
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
use_condaenv("r-tensorflow")
sess = tf$Session()
devtools::install_github("rstudio/reticulate")
install.packages("skimr")
install.packages("recipes")
install.packages("data.table")
install.packages("PerformanceAnalytics")
devtools::install_github("Rdatatable/data.table")
devtools::install_github("Rdatatable/data.table")
install.packages("data.table", type="source")
# Standard packages
library(tidyverse) # All the tidy stuff
library(data.table) # For fast working with big data, and easy loading
install.packages(data.table)
install.packages("data.table")
install.packages("data.table")
install.packages("data.table", type="source")
?step_log
# Create recipe -> new package to make preprocessing really easy
require(recipes)
?step_log
plot(cars)
plot(cars)
summary(cars)
### Generic preamble
Sys.setenv(LANG = "en")
set.seed(1337)
### Install packages if necessary
list.of.packages <- c("devtools", "rstudioapi", "knitr", "data.table", "skimr", "caret", "caTools", "caretEnsemble")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages)
### Clean Workspace (I like to start clean)
rm(list=ls()); graphics.off()
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base","recipes")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
### Load extrapackages prediction
library(caret) # ML training
library(caTools) # Some extra caret tools
library(caretEnsemble) # To work with ensembles
library(recipes) # For nice preprocessing workflows
# Knitr options
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
# pimp up memory
memory.limit(10 * 10^10)
data <- readRDS("input/patents_composite.RDS")
glimpse(data)
# initial limitation
data <- setDT(data)[filing >= 2010 & filing <= 2015 & source == "ustpo"]
rank <- data %>%
filter(fwd_cits5 > 0) %>%
group_by(filing) %>%
mutate(fwd_cits5_quant = percent_rank(fwd_cits5)) %>%
ungroup() %>%
select(appln_id, fwd_cits5_quant)
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough05))
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough05))
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough50))
# label the levels
levels(data$breakthrough) <- c("no", "yes"); levels(data$breakthrough50) <- c("no", "yes")
rm(rank)
# Further preprocessing
data <- data %>%
arrange(appln_id) %>%
column_to_rownames("appln_id") %>%
replace_na(list(tech_field = 99, grant_lag = 0, patent_scope = 1, claims_bwd = 0, originality = 0, radicalness = 0, nb_applicants = 1, nb_inventors = 1)) %>%
select(-appln_nbr, -appln_auth, -source, -filing, -claims, -fwd_cits5, -fwd_cits7, -generality, -renewal, -quality_index_4, -quality_index_6, -fwd_cits5_quant) %>%
drop_na() %>%
select(breakthrough, breakthrough50, everything()) %>%
glimpse()
### Save for later
saveRDS(data, "temp/data_reg.RDS")
?rapply
### Generic preamble
Sys.setenv(LANG = "en")
### Clean Workspace (I like to start clean)
rm(list=ls()); graphics.off() # get rid of everything in the workspace
detachAllPackages <- function() { # Also, detach packages to avoid functions masked by others
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(extrafont)
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr)
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
library(knitr)
### pimp up memory (to save on disk if necessary, only works on windows)
#memory.limit(10 * 10^10)
### Knitr options
opts_chunk$set(warning = FALSE,
message = FALSE,
echo    = FALSE,
fig.align  = "center"
)
list.of.packages <- c("knitr",
"data.table",
"caret",
"caretEnsemble",
"recipes"
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages)
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
View(data)
is.factor(data)
is.factor(data[,])
is_character(data)
is_character(data[1:n()])
is_character(data[,1:ncol(data)])
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor() %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
View(data)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_predictors(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_nominal(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data
glimpse(data)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-tenure, -SeniorCitizen, -MonthlyCharges, -TotalCharges) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data <- readRDS("german_credit_data.csv")
data <- readRDS("german_credit_data.csv")
data <- readRDS("data/german_credit_data.csv")
setwd("C:/Users/Admin/OneDrive - Aalborg Universitet/01 - Teaching/Course - SDS/00_github/M1-2018")
data <- readRDS("data/german_credit_data.csv")
data <- fread("data/german_credit_data.csv")
glimpse(data)
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-tenure, -SeniorCitizen, -MonthlyCharges, -TotalCharges) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_predictors(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
glimpse(data)
### Generic preamble
Sys.setenv(LANG = "en")
### Clean Workspace (I like to start clean)
rm(list=ls()); graphics.off() # get rid of everything in the workspace
detachAllPackages <- function() { # Also, detach packages to avoid functions masked by others
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(extrafont)
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr)
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
library(knitr)
### pimp up memory (to save on disk if necessary, only works on windows)
#memory.limit(10 * 10^10)
### Knitr options
opts_chunk$set(warning = FALSE,
message = FALSE,
echo    = FALSE,
fig.align  = "center"
)
list.of.packages <- c("knitr",
"data.table",
"caret",
"caretEnsemble",
"recipes"
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages)
# Read data
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Split in x/y & train/test
x_train <- bake(reci, newdata = training) %>% select(-Churn)
x_test  <- bake(reci, newdata = test) %>% select(-Churn)
y_train <- pull(training, Churn) %>% as.factor()
y_test  <- pull(test, Churn) %>% as.factor()
# Remove what we dont need anymore
rm(data, index, training, test, reci)
fit.rf <- train(x = x_train,
y = y_train,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, classProbs = TRUE,  summaryFunction = twoClassSummary,
returnData = FALSE, returnResamp = "final", savePredictions = "final", verboseIter = FALSE)
metric <- "ROC" # Which metric should be optimized (more on that later)
fit.rf <- train(x = x_train,
y = y_train,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
vip(fit.rf) + ggtitle("ranger: RF")
install.packages("vip")
library(vip)
vip(fit.rf) + ggtitle("ranger: RF")
library(vip)
vip(fit.rf) + ggtitle("VarImp: Random Forest")
library(lime)
?explainer
?lime
# Read data
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Split in x/y & train/test
training <- bake(reci, newdata = training)
test  <- bake(reci, newdata = test)
# Remove what we dont need anymore
rm(data, index, reci)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, classProbs = TRUE,  summaryFunction = twoClassSummary,
returnData = FALSE, returnResamp = "final", savePredictions = "final", verboseIter = FALSE)
metric <- "ROC" # Which metric should be optimized (more on that later)
fit.rf <- train(Churn ~ .,
data = training,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
library(lime)
expl.rf<- lime(training, fit.rf)
head(explanation)
head(expl.rf)
expl.rf<- lime(test[1:5], fit.rf)
head(expl.rf)
plot_features(expl.rf)
library(lime)
library(lime)
install.packages(lime)
install.packages("lime")
install.packages("lime")
library(lime)
expl.rf<- lime(test[1:5], fit.rf)
head(expl.rf)
