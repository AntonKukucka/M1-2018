rm(list=ls()); graphics.off()
detachAllPackages <- function() {
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base","recipes")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
### Load extrapackages prediction
library(caret) # ML training
library(caTools) # Some extra caret tools
library(caretEnsemble) # To work with ensembles
library(recipes) # For nice preprocessing workflows
# Knitr options
opts_knit$set(root.dir = dirname(rstudioapi::getActiveDocumentContext()$path))
# pimp up memory
memory.limit(10 * 10^10)
data <- readRDS("input/patents_composite.RDS")
glimpse(data)
# initial limitation
data <- setDT(data)[filing >= 2010 & filing <= 2015 & source == "ustpo"]
rank <- data %>%
filter(fwd_cits5 > 0) %>%
group_by(filing) %>%
mutate(fwd_cits5_quant = percent_rank(fwd_cits5)) %>%
ungroup() %>%
select(appln_id, fwd_cits5_quant)
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough05))
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough05))
data <- left_join(data, rank, by= "appln_id") %>%
replace_na(list(fwd_cits5_quant = 0)) %>%
mutate(breakthrough50 = as.numeric(fwd_cits5_quant >= 0.50) ) %>%
replace_na(list(breakthrough = 0, breakthrough50 = 0)) %>%
mutate(breakthrough = factor(breakthrough), breakthrough50 = factor(breakthrough50))
# label the levels
levels(data$breakthrough) <- c("no", "yes"); levels(data$breakthrough50) <- c("no", "yes")
rm(rank)
# Further preprocessing
data <- data %>%
arrange(appln_id) %>%
column_to_rownames("appln_id") %>%
replace_na(list(tech_field = 99, grant_lag = 0, patent_scope = 1, claims_bwd = 0, originality = 0, radicalness = 0, nb_applicants = 1, nb_inventors = 1)) %>%
select(-appln_nbr, -appln_auth, -source, -filing, -claims, -fwd_cits5, -fwd_cits7, -generality, -renewal, -quality_index_4, -quality_index_6, -fwd_cits5_quant) %>%
drop_na() %>%
select(breakthrough, breakthrough50, everything()) %>%
glimpse()
### Save for later
saveRDS(data, "temp/data_reg.RDS")
?rapply
### Generic preamble
Sys.setenv(LANG = "en")
### Clean Workspace (I like to start clean)
rm(list=ls()); graphics.off() # get rid of everything in the workspace
detachAllPackages <- function() { # Also, detach packages to avoid functions masked by others
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(extrafont)
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr)
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
library(knitr)
### pimp up memory (to save on disk if necessary, only works on windows)
#memory.limit(10 * 10^10)
### Knitr options
opts_chunk$set(warning = FALSE,
message = FALSE,
echo    = FALSE,
fig.align  = "center"
)
list.of.packages <- c("knitr",
"data.table",
"caret",
"caretEnsemble",
"recipes"
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages)
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
View(data)
is.factor(data)
is.factor(data[,])
is_character(data)
is_character(data[1:n()])
is_character(data[,1:ncol(data)])
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor() %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
View(data)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_predictors(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_nominal(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data
glimpse(data)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-tenure, -SeniorCitizen, -MonthlyCharges, -TotalCharges) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
data <- readRDS("german_credit_data.csv")
data <- readRDS("german_credit_data.csv")
data <- readRDS("data/german_credit_data.csv")
setwd("C:/Users/Admin/OneDrive - Aalborg Universitet/01 - Teaching/Course - SDS/00_github/M1-2018")
data <- readRDS("data/german_credit_data.csv")
data <- fread("data/german_credit_data.csv")
glimpse(data)
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-tenure, -SeniorCitizen, -MonthlyCharges, -TotalCharges) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(-all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_string2factor(all_predictors(), -all_numeric()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
glimpse(data)
### Generic preamble
Sys.setenv(LANG = "en")
### Clean Workspace (I like to start clean)
rm(list=ls()); graphics.off() # get rid of everything in the workspace
detachAllPackages <- function() { # Also, detach packages to avoid functions masked by others
basic.packages <- c("package:stats","package:graphics","package:grDevices","package:utils","package:datasets","package:methods","package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:",search()))==1,TRUE,FALSE)]
package.list <- setdiff(package.list,basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package, character.only=TRUE)
}
detachAllPackages(); rm(detachAllPackages)
### Load packages  Standard
library(extrafont)
library(knitr) # For display of the markdown
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr)
library(data.table) # Good format to work with large datasets
library(skimr) # Nice descriptives
library(knitr)
### pimp up memory (to save on disk if necessary, only works on windows)
#memory.limit(10 * 10^10)
### Knitr options
opts_chunk$set(warning = FALSE,
message = FALSE,
echo    = FALSE,
fig.align  = "center"
)
list.of.packages <- c("knitr",
"data.table",
"caret",
"caretEnsemble",
"recipes"
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(list.of.packages)
# Read data
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Split in x/y & train/test
x_train <- bake(reci, newdata = training) %>% select(-Churn)
x_test  <- bake(reci, newdata = test) %>% select(-Churn)
y_train <- pull(training, Churn) %>% as.factor()
y_test  <- pull(test, Churn) %>% as.factor()
# Remove what we dont need anymore
rm(data, index, training, test, reci)
fit.rf <- train(x = x_train,
y = y_train,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, classProbs = TRUE,  summaryFunction = twoClassSummary,
returnData = FALSE, returnResamp = "final", savePredictions = "final", verboseIter = FALSE)
metric <- "ROC" # Which metric should be optimized (more on that later)
fit.rf <- train(x = x_train,
y = y_train,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
vip(fit.rf) + ggtitle("ranger: RF")
install.packages("vip")
library(vip)
vip(fit.rf) + ggtitle("ranger: RF")
library(vip)
vip(fit.rf) + ggtitle("VarImp: Random Forest")
library(lime)
?explainer
?lime
# Read data
data <- readRDS("data/telco_churn.rds")
# Mini manual preprocessing
data %<>%
select(-customerID) %>%
drop_na() %>%
select(Churn, everything())
# Split in training and test
library(caret)
index <- createDataPartition(y = data$Churn, p = 0.75, list = FALSE)
training <- data[index,]
test <- data[-index,]
# Do the preprocessing recipe
library(recipes)
reci <- recipe(Churn ~ ., data = training) %>%
step_discretize(tenure, options = list(cuts = 6)) %>%
step_log(TotalCharges) %>%
step_dummy(all_nominal(), -all_outcomes()) %>%
step_center(all_predictors(), -all_outcomes()) %>%
step_scale(all_predictors(), -all_outcomes()) %>%
prep(data = training)
# Split in x/y & train/test
training <- bake(reci, newdata = training)
test  <- bake(reci, newdata = test)
# Remove what we dont need anymore
rm(data, index, reci)
ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, classProbs = TRUE,  summaryFunction = twoClassSummary,
returnData = FALSE, returnResamp = "final", savePredictions = "final", verboseIter = FALSE)
metric <- "ROC" # Which metric should be optimized (more on that later)
fit.rf <- train(Churn ~ .,
data = training,
trControl = ctrl,
metric = metric,
method = "ranger",
importance = "impurity",
tuneLength = 10,
num.trees = 25)
library(lime)
expl.rf<- lime(training, fit.rf)
head(explanation)
head(expl.rf)
expl.rf<- lime(test[1:5], fit.rf)
head(expl.rf)
plot_features(expl.rf)
library(lime)
library(lime)
install.packages(lime)
install.packages("lime")
install.packages("lime")
library(lime)
expl.rf<- lime(test[1:5], fit.rf)
head(expl.rf)
rm(list=ls())
### Installing packages: If you did not install the packages, uncomment (remove the leading #) the following lines and run
install.packages("tidyverse")
install.packages("data.table")
### Loading packages
library(tidyverse) # loads the whole tidyverse, including dplyr, readr, ggplot, etc
library(data.table) # loads the data.table package, in case you use the fread()
# Necessary: the datafiles you find in the assignment folder have to be placed in the same folder as this script file.
# Best thing to do: fread() from data.table
# header = TRUE tells fread that the first row should be interpreted as variable-name
# data.table = TRUE tells fread that it should not create a data.table (its own data format), but a dataframe instead
# check.names = TRUE will replace space in Variable names with . (because R doesnt like that)
cities_df <- fread("cities_df.csv", header = TRUE, check.names = TRUE, data.table = FALSE)
trips_df <- fread("trips_df.csv", header = TRUE, check.names = TRUE, data.table = FALSE)
head(cities_df)
head(trips_df)
# variable names are a mix of capital letters, and small letters. Lets make them all small.
colnames(trips_df) <- tolower(colnames(trips_df))
colnames(cities_df) <- tolower(colnames(cities_df))
# Further, we see that for some reasons some variable names (eg., country) appear more than once
# Let's throw them out
cities_df <- cities_df[,!duplicated(colnames(cities_df))]
#And I will create copy of the full dataset for future analysis
cities_full <- cities_df
### Lets only look at cities:
glimpse(cities_df)
# There are 79 variables and I am not planning to recode or use all of them for my model, just a few, representing different characteristics of cities.
#I have chosen my own set of variables to inspect and saved the default dataframe as cities2 for reference:
cities_df <- cities_df %>%
select(place_slug, friendly.to.foreigners, safety, quality.of.life, fun, cost.of.living, english.speaking, places.to.work.from)
glimpse(cities_df)
# Lets recode the characters into numeric. Lets only do it for "fun"
# First, check what are the different values "fun" can take.
unique(cities_df$fun)
# Don't worry about the warning. Only states that the not mentioned values (here, an empty string "") will be replaced by NA
cities_df <- cities_df %>%
mutate(fun = recode(fun, bad = 1, okay = 2, good = 3, great = 4))
# We also could do that for all other variables.
# Since they are coded in the same way (bad, okay, good, great), so we can use the same code, only change the variable name
cities_df <- cities_df %>%
mutate(friendly.to.foreigners = recode(friendly.to.foreigners, bad = 1, okay = 2, good = 3, great = 4),
safety = recode(safety, bad = 1, okay = 2, good = 3, great = 4),
quality.of.life = recode(quality.of.life, bad = 1, okay = 2, good = 3, great = 4),
cost.of.living = recode(cost.of.living, bad = 1, okay = 2, good = 3, great = 4),
english.speaking = recode(english.speaking, bad = 1, okay = 2, good = 3, great = 4),
places.to.work.from = recode(places.to.work.from, bad = 1, okay = 2, good = 3, great = 4))
# Lets recode the characters into numeric. Lets only do it for "fun"
# First, check what are the different values "fun" can take.
unique(cities_df$fun)
# Don't worry about the warning. Only states that the not mentioned values (here, an empty string "") will be replaced by NA
cities_df <- cities_df %>%
mutate(fun = recode(fun, bad = 1, okay = 2, good = 3, great = 4))
# We also could do that for all other variables.
# Since they are coded in the same way (bad, okay, good, great), so we can use the same code, only change the variable name
cities_df <- cities_df %>%
mutate(friendly.to.foreigners = recode(friendly.to.foreigners, bad = 1, okay = 2, good = 3, great = 4),
safety = recode(safety, bad = 1, okay = 2, good = 3, great = 4),
quality.of.life = recode(quality.of.life, bad = 1, okay = 2, good = 3, great = 4),
cost.of.living = recode(cost.of.living, bad = 1, okay = 2, good = 3, great = 4),
english.speaking = recode(english.speaking, bad = 1, okay = 2, good = 3, great = 4),
places.to.work.from = recode(places.to.work.from, bad = 1, okay = 2, good = 3, great = 4))
# Now we just drop all NAs for this exercise
cities_df <- cities_df %>%
drop_na()
cities_full <- cities_full %>%
drop_na()
# Further, we drop cities that appear twice. Argument ".keep_all = TRUE" means that we do not want to discard the rest of the variables
cities_df <- cities_df %>%
distinct(place_slug, .keep_all = TRUE)
# Note: for graphical visualization, we need rownames. Its outdated and silly, but cannot be changed, since the plot function labels by rownames
rownames(cities_df) <- cities_df[,"place_slug"]
rownames(cities_full) <- cities_full[,"place_slug"]
# kmeans() is the easiest one, included in base R. More clustering techniques can be found in the "FactoMineR" package
# we only select the variables we want to cluster
# The argument "centers = 3" specifies how many clusters we would like to have
#
# Trick: Don't forget to scale() the data
?kmeans
set.seed(321) #to keep the same cluster numbers as for my analysis
km.cities <- kmeans(cities_df %>% select(-place_slug) %>% scale(), centers = 3)
# we could visualize it with the "factoextra" package
library(factoextra)
fviz_cluster(km.cities,
data = cities_df %>% select(-place_slug) %>% scale())
analysis1 <- subset(cities_df[
c("dammam-saudi-arabia","jeddah-saudi-arabia","bogota-colombia","florianopolis-brazil","makati-philippines","monaco-monaco","adelaide-australia","deventer-netherlands","coventry-united-kingdom"),
c(2:8)])
analysis1
# Clustering does not change the order, so we can just bind it.
cities_df <- cities_df %>%
bind_cols(cluster = km.cities$cluster)
#And check correlation matrix
vars <- c(cities_df[2:8])
library(GGally)
install.packages("GGally")
# Clustering does not change the order, so we can just bind it.
cities_df <- cities_df %>%
bind_cols(cluster = km.cities$cluster)
#And check correlation matrix
vars <- c(cities_df[2:8])
library(GGally)
ggcorr(vars, label = TRUE, label_size = 3, label_round = 2, label_alpha = TRUE)
# We could see which cluster was more visited, therefore may be more popular
# Easiest way is to join it with the trips and count how many times there where trips to the cluster
# We know that the variable "place_slug" appears in bost dataframes, so can be used as ID for a merge
trips_df <- trips_df %>%
left_join(cities_df %>% select(place_slug, cluster), by = "place_slug")
# Now, lets see how many people visited the different clusters overall
trips_df %>%
group_by(cluster) %>%
count()
#There are many missing values for clusters in trips dataset, so I decided to drop those observations for further analysis
trips_df <- trips_df %>%
drop_na()
#It is also better to convert dates of trips to proper format, recognized by R
library(lubridate)
trips_df$date_end <- ymd(trips_df$date_end)
trips_df$date_start <- as.Date(trips_df$date_start)
#And I can check whether my code worked or not
class(trips_df$date_end)
class(trips_df$date_start)
start <- c(trips_df$date_start)
end <- c(trips_df$date_end)
elapsed.time <- start %--% end
duration <- data.frame(as.duration(elapsed.time) / ddays(1))
#I created new table to check if the counts for trips make any sense but also to look only at chosen variables from trips dataframe
analysis2 <- data.frame(trips_df$place_slug, trips_df$date_start, trips_df$date_end, duration, trips_df$cluster)
colnames(analysis2) <- c("place_slug","start_date","end_date","duration","cluster")
#Next step is subsetting newly created dataframe based on which cluster were nomads travelling to, in order to count for how long were they staying in cities and see if there's any significant difference between stays.
cluster1 <-subset(analysis2, cluster==1) #the red cluster
cluster2 <-subset(analysis2, cluster==2) #the green cluster
cluster3 <-subset(analysis2, cluster==3) #the blue cluster
#I also counted the values once again, just to make sure that it matches my subsets (I don't trust my coding skills :P)
analysis2 %>%
group_by(cluster) %>%
count()
summarize(cluster1, red = mean(cluster1$duration, na.rm = TRUE))
cluster1 <-subset(cluster1, start_date>1970-01-01 & duration>=1)
cluster2 <-subset(cluster2, start_date>1970-01-01 & duration>=1)
cluster3 <-subset(cluster3, start_date>1970-01-01 & duration>=1)
#I decided to look at the trips at least 1 day long and started after 1970, as I noticed that some users registered old travelling experiences as well
summarize(cluster1, red = mean(cluster1$duration, na.rm = TRUE))
summarize(cluster2, green = mean(cluster2$duration, na.rm = TRUE))
summarize(cluster3, blue = mean(cluster3$duration, na.rm = TRUE))
cluster1 <-subset(cluster1, duration>=7 & duration<=365)
cluster2 <-subset(cluster2, duration>=7 & duration<=365)
cluster3 <-subset(cluster3, duration>=7 & duration<=365)
summarize(cluster1, red = mean(cluster1$duration, na.rm = TRUE))
summarize(cluster2, green = mean(cluster2$duration, na.rm = TRUE))
summarize(cluster3, blue = mean(cluster3$duration, na.rm = TRUE))
# Now I will try to see how diverse regions are, using the full dataset of cities that I saved copy of beforehand.
cities_full <- cities_full %>%
left_join(cities_df %>% select(place_slug, cluster), by = "place_slug")
analysis3 <- data.frame(cities_full$region, cities_full$cluster)
analysis3 <- analysis3 %>%
drop_na()
colnames(analysis3) <- c("region","cluster")
counts <- table(analysis3$cluster, analysis3$region)
barplot(counts, main="Clusters distribution",
xlab="Regions", col=c("red","green","blue"),
legend = rownames(counts))
